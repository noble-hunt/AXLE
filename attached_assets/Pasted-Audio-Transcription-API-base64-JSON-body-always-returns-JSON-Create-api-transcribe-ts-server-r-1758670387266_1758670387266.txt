Audio Transcription API (base64 JSON body; always returns JSON)

Create

api/transcribe.ts

server/routes/transcribe.ts (mount at /api/transcribe)

// api/transcribe.ts  (Vercel serverless, Node runtime)
import { openai } from './_openai';
export const config = { runtime: 'nodejs18.x' };

export default async function handler(req: Request) {
  if (req.method !== 'POST') {
    return new Response(JSON.stringify({ error: 'Method Not Allowed' }), {
      status: 405, headers: { 'content-type': 'application/json' }
    });
  }

  type Body = { audioBase64: string; mimeType?: string; language?: string };
  let body: Body;
  try { body = await req.json(); }
  catch { return json({ error: 'Invalid JSON body' }, 400); }

  if (!body?.audioBase64) return json({ error: 'audioBase64 required' }, 400);
  if (!process.env.OPENAI_API_KEY) return json({ error: 'OPENAI_API_KEY missing' }, 500);

  try {
    const buffer = Buffer.from(body.audioBase64, 'base64');
    // Undici's File is global in Node 18+
    const file = new File([buffer], `audio.${guessExt(body.mimeType)}`, { type: body.mimeType || 'audio/webm' });

    const r = await openai.audio.transcriptions.create({
      file,
      model: 'whisper-1',
      language: body.language || 'en'
    });

    return json({ text: r.text ?? '' }, 200);
  } catch (e: any) {
    console.error('[transcribe] error', e);
    return json({ error: e?.message || 'transcription failed' }, 500);
  }

  function guessExt(mt?: string){ if(!mt) return 'webm'; if(mt.includes('mp4')) return 'mp4'; if(mt.includes('mpeg')) return 'mp3'; if(mt.includes('wav')) return 'wav'; if(mt.includes('ogg')) return 'ogg'; return 'webm';}
  function json(x:any, status=200){ return new Response(JSON.stringify(x), { status, headers:{'content-type':'application/json','cache-control':'no-store'} }); }
}

// server/routes/transcribe.ts (Express dev)
import { Router } from 'express';
import { openai } from '../lib/openai';
const router = Router();

router.post('/', async (req, res) => {
  const { audioBase64, mimeType, language } = req.body ?? {};
  if (!audioBase64) return res.status(400).json({ error: 'audioBase64 required' });
  try {
    const buffer = Buffer.from(audioBase64, 'base64');
    const file = new File([buffer], `audio.webm`, { type: mimeType || 'audio/webm' });
    const r = await openai.audio.transcriptions.create({
      file, model: 'whisper-1', language: language || 'en'
    });
    res.json({ text: r.text ?? '' });
  } catch (e: any) {
    console.error('[dev/transcribe] err', e);
    res.status(500).json({ error: e?.message || 'transcription failed' });
  }
});

export default router;

// server/index.ts  (mount once)
import transcribeRouter from './routes/transcribe';
app.use('/api/transcribe', transcribeRouter);


Why this fixes your prod error

No multipart parsing in serverless needed; body is JSON (safe everywhere).

We always return JSON with content-type: application/json, so the client’s res.json() never hits “Unexpected end of JSON input”.